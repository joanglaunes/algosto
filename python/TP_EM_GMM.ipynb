{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5a81ffb9-2e48-412f-9287-55a3d06ed617",
      "metadata": {
        "id": "5a81ffb9-2e48-412f-9287-55a3d06ed617"
      },
      "source": [
        "Ouvrir ce notebook dans :\n",
        "<a href=\"https://colab.research.google.com/github/joanglaunes/algosto/blob/master/python/TP_EM_GMM.ipynb\" target=\"_blank\">Google Colab</a>\n",
        "ou\n",
        "<a href=\"https://rosenblatt.ens.math-info.univ-paris5.fr/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fjoanglaunes%2Falgosto&urlpath=tree%2Falgosto%2Fpython%2FTP_EM_GMM.ipynb&branch=master\" target=\"_blank\">Rosenblatt</a>\n",
        "\n",
        "# TP Algorithmes EM et SEM pour mélanges de gaussiennes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3688bee-bce6-47eb-b678-f3678459ca76",
      "metadata": {
        "id": "e3688bee-bce6-47eb-b678-f3678459ca76"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.linalg import sqrtm\n",
        "import scipy.stats as sps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc98dde0-2c82-43af-90e7-f0347c90cc3f",
      "metadata": {
        "id": "cc98dde0-2c82-43af-90e7-f0347c90cc3f"
      },
      "outputs": [],
      "source": [
        "#from TP_EM_GMM import *"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abb4a45e-a80c-438e-af39-8ef6cc27af37",
      "metadata": {
        "id": "abb4a45e-a80c-438e-af39-8ef6cc27af37"
      },
      "source": [
        "$\\newcommand{\\R}{\\mathbb{R}}$\n",
        "\n",
        "## Rappels de cours\n",
        "\n",
        "### Modèle de mélange gaussien\n",
        "Un mélange gaussien en dimension $d$ est une loi de densité\n",
        "$$f_\\theta(x) = \\sum_{k=1}^K \\pi_k f_{\\mu_k,\\Sigma_k}(x),$$\n",
        "où les poids $\\pi_k$ forment un vecteur de probabilité, et la notation $f_{\\mu,\\Sigma}$ désigne la densité de la loi normale multidimensionnelle d'espérance $\\mu\\in\\R^d$ et de matrice de covariance $\\Sigma\\in\\R^{d\\times d}$. On note $\\theta=(\\pi_k,\\mu_k,\\Sigma_k)_{1\\leq k\\leq K}$ l'ensemble des paramètres de ce mélange gaussien.\n",
        "\n",
        "### Simulation des données\n",
        "\n",
        "Pour simuler un échantillon d'une loi de mélange gaussien, on commence par simuler un échantillon $(Z_1,\\ldots,Z_n)$ de loi donnée par $\\mathbb{P}[Z_i=k]=\\pi_k$ pour tout $k\\in\\{1,\\ldots,K\\}$, puis pour chaque $i$ on simule $X_i$ selon la loi de densité $f_{\\theta_k}$, où $k=z_i$, valeur simulée de $Z_i$.\n",
        "\n",
        "<font color='blue'> 1. Ecrire une fonction `sample_gaussian_mixture(n, pi, mu, Sigma)` permettant de simuler $n$ réalisations indépendantes d'une telle loi. *pi* est le vecteur des paramètres du mélange, de longueur *K*, *mu* les moyennes, données sous forme de tableau de taille (K,d), et *Sigma* les matrices de covariance, tableau de taille $(K,d,d)$. La fonction doit renvoyer à la fois les $x_i$ et les $z_i$. On pourra utiliser la fonction `numpy.random.multivariate_normal`.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dc03f78-129e-45ac-8cb4-b3a41be07395",
      "metadata": {
        "id": "9dc03f78-129e-45ac-8cb4-b3a41be07395"
      },
      "outputs": [],
      "source": [
        "# 1. fonction pour simuler un mélange gaussien\n",
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='blue'> 2. Tester cette fonction d'abord en dimension $d=1$, avec $K=3$, $\\pi=(.4,.3,.3)$, $\\mu=(-4,4,0)$, $\\Sigma=(1,1,1)$ et $n = 10000$ et tracer l'histogramme des réalisations ainsi obtenues.</font>"
      ],
      "metadata": {
        "id": "LRpNLp62dR0G"
      },
      "id": "LRpNLp62dR0G"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67d925d9-5a69-4a2f-a5d6-ee1c9187d7f1",
      "metadata": {
        "id": "67d925d9-5a69-4a2f-a5d6-ee1c9187d7f1"
      },
      "outputs": [],
      "source": [
        "# 2. test en dimension 1\n",
        "# TODO\n",
        "question_2()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e530a361-a512-4271-bd9c-87e6144b6e99",
      "metadata": {
        "id": "e530a361-a512-4271-bd9c-87e6144b6e99"
      },
      "source": [
        "Voici à présent un exemple en dimension 2. Définissons d'abord une fonction d'affichage qui nous sera utile pour toute la suite:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3fe87a3-a3b8-4f55-98a4-06e2a662f773",
      "metadata": {
        "id": "c3fe87a3-a3b8-4f55-98a4-06e2a662f773"
      },
      "outputs": [],
      "source": [
        "def plot_gaussian_mixture_2d(mu, Sigma, x=None, z=None, axis_equal=True, title=\"\"):\n",
        "    K = mu.shape[0]\n",
        "    if x is not None:\n",
        "        c = \"k\" if z is None else z\n",
        "        plt.scatter(x[:,0], x[:,1], s=.1, c=c)\n",
        "    mu = mu[:,None,:,None]                            # (K,1,2,1)\n",
        "    rSigma = np.array([sqrtm(Sigma[k]) for k in range(K)])\n",
        "    rSigma = rSigma[:,None,:,:]                       # (K,1,2,2)\n",
        "    t = np.linspace(0,2*np.pi)[None,:,None,None]      # (1,n,1,1)\n",
        "    y = np.concatenate((np.cos(t),np.sin(t)),axis=2)  # (1,n,2,1)\n",
        "    y = 2*rSigma @ y + mu                             # (K,n,2,1)\n",
        "    plt.plot(y[:,:,0,0].T,y[:,:,1,0].T)\n",
        "    if axis_equal:\n",
        "        plt.axis(\"equal\")\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ace9285c-af30-4d3c-9d82-bb1fc91b43c9",
      "metadata": {
        "id": "ace9285c-af30-4d3c-9d82-bb1fc91b43c9"
      },
      "source": [
        "On teste à présent la fonction `sample_gaussian_mixture` pour $d=2$, d'abord dans un cas simple, avec $K=2$ et des gaussiennes de covariance égales à la matrice identité mais de moyennes différentes, et on affiche:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1e712a7-109c-4c17-b81e-1dc49606a148",
      "metadata": {
        "id": "d1e712a7-109c-4c17-b81e-1dc49606a148"
      },
      "outputs": [],
      "source": [
        "n = 10000\n",
        "pi = np.array([.5,.5])\n",
        "mu = np.array([[2,2],[-2,-2]])\n",
        "Sigma = np.array([np.eye(2),np.eye(2)])\n",
        "x, z = sample_gaussian_mixture(n, pi, mu, Sigma)\n",
        "plot_gaussian_mixture_2d(mu,Sigma,x,z)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c68281d-83a0-4edf-b6b0-6d1dd4446c26",
      "metadata": {
        "id": "2c68281d-83a0-4edf-b6b0-6d1dd4446c26"
      },
      "source": [
        "A présent on teste un cas un peu plus compliqué avec $K=4$ et des matrices de covariances non identiques. Ce cas nous servira ensuite pour tester les algorithmes EM:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5de8c3d5-5446-4320-9aed-7223d24dfca8",
      "metadata": {
        "id": "5de8c3d5-5446-4320-9aed-7223d24dfca8"
      },
      "outputs": [],
      "source": [
        "n = 10000\n",
        "pi = [.25,.25,.25,.25]\n",
        "mu = np.array([[2,2],[-2,-2],[2,-2],[-2,2]])\n",
        "P, Q = np.array([[2,1],[-1,0]]), np.array([[-2,1],[-1,-1]])\n",
        "Sigma = np.array([np.eye(2),np.eye(2),P@P.T,Q@Q.T])\n",
        "x, z = sample_gaussian_mixture(n, pi, mu, Sigma)\n",
        "plot_gaussian_mixture_2d(mu,Sigma,x,z)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f35f5f2f-8193-4038-8c2b-2bf227e7e44c",
      "metadata": {
        "id": "f35f5f2f-8193-4038-8c2b-2bf227e7e44c"
      },
      "source": [
        "### Algorithme EM\n",
        "\n",
        "L'algorithme EM permet d'estimer les paramètres d'un mélange gaussien. On rappelle ici ses étapes:\n",
        "- On se donne en entrée des observations $\\mathbf{x} = (x_1,\\dots,x_n)$, supposées être des réalisations i.i.d. des $X_i$, et des paramètres initiaux\n",
        "$\\theta^0 = (\\pi_k^0,\\mu_k^0,\\Sigma_k^0)_{1\\leq k\\leq K} $\n",
        "- Tant qu'on n'a pas convergé, itérer :\n",
        "  -  __Etape E__: calculer pour tout $i$ dans $\\{1,\\dots,n\\}$ et\n",
        "tout $k$ dans $\\{1,\\dots,K\\}$ les valeurs\n",
        "$$\\gamma_{ik}^{t} =\\frac{\\pi_{k}^t g_{\\mu_{k}^t, \\Sigma_k^t}(x_i)}{\\sum_{j=1}^K\\pi_{j}^tg_{\\mu_{j}^t,\\Sigma_j^t}(x_i)}.$$\n",
        "  - __Etape M__:  mettre à jour les paramètres\n",
        "\\begin{align*} \\mu_k^{t+1} &= \\frac{1}{\\sum_{i=1}^n \\gamma_{ik}^{t}}\\sum_{i=1}^n\n",
        " \\gamma_{ik}^t x_i.\n",
        " \\\\\\Sigma_k^{t+1} &= \\frac{1}{\\sum_{i=1}^n \\gamma_{ik}^t}\\sum_{i=1}^n\n",
        "  \\gamma_{ik}^t (x_i- \\mu_k^{t+1}) (x_i- \\mu_k ^{t+1})^T,\n",
        "  \\\\\\pi_k^{t+1} &= \\frac 1 n \\sum_{i=1}^n \\gamma_{ik}^t.\\end{align*}\n",
        "\n",
        "Voici d'abord une fonction pour définir des paramètres initiaux:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5143cdd-477b-4529-a8db-1e611f3e7405",
      "metadata": {
        "id": "f5143cdd-477b-4529-a8db-1e611f3e7405"
      },
      "outputs": [],
      "source": [
        "def init_EM(x,K):\n",
        "    n, d = x.shape\n",
        "    pi0 = np.ones(K)/K\n",
        "    mu0 = np.mean(x,axis=0)[None,:] + np.random.randn(K,d) * np.std(x,axis=0)[None,:]\n",
        "    Sigma0 = np.repeat(np.eye(2)[None,:,:],K,axis=0)\n",
        "    return pi0, mu0, Sigma0\n",
        "\n",
        "K = 4\n",
        "pi0, mu0, Sigma0 = init_EM(x,K)\n",
        "plot_gaussian_mixture_2d(mu0,Sigma0,x, title=\"Les données xi et le mélange gaussien initial\");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6917fb09-f3b8-4d44-9b8c-2f79419c7a35",
      "metadata": {
        "id": "6917fb09-f3b8-4d44-9b8c-2f79419c7a35"
      },
      "source": [
        "<font color='blue'> 3. Ecrire une fonction `EM_gaussian_mixture(x, K, niter=50)` permettant d'effectuer *niter* itérations de l'algorithme EM pour les données $x_i$ et le nombre de composantes $K$. Les paramètres initiaux seront choisis avec la fonction `init_EM`, et on pourra utiliser la fonction `scipy.stats.multivariate_normal.pdf`</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2828ed16-f9fe-45a8-ac08-45d87ad54a5f",
      "metadata": {
        "id": "2828ed16-f9fe-45a8-ac08-45d87ad54a5f"
      },
      "outputs": [],
      "source": [
        "# 3. fonction pour l'algorithme EM\n",
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f2f650f-2901-4024-abb2-dd5b3d56c1cc",
      "metadata": {
        "id": "0f2f650f-2901-4024-abb2-dd5b3d56c1cc"
      },
      "source": [
        "On teste à présent l'algorithme sur les données simulées précédentes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd1d7214-b7a9-4154-9c29-b45734c3fe6a",
      "metadata": {
        "id": "dd1d7214-b7a9-4154-9c29-b45734c3fe6a"
      },
      "outputs": [],
      "source": [
        "K = 4\n",
        "pi, mu, Sigma, gamma = EM_gaussian_mixture(x, K)\n",
        "\n",
        "# affichage\n",
        "z = np.argmax(gamma,axis=0)\n",
        "plot_gaussian_mixture_2d(mu,Sigma,x,z, title=\"Résultat de l'algorithme EM\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3862710a-f6b6-418c-9989-72f068b15043",
      "metadata": {
        "id": "3862710a-f6b6-418c-9989-72f068b15043"
      },
      "source": [
        "### Algorithme SEM\n",
        "\n",
        "$\\newcommand{\\one}{\\mathbb{1}}$\n",
        "\n",
        "L'algorithme SEM (Stochastic EM) est une variante introduisant une composante aléatoire dans les itérations.\n",
        "Après le calcul des $\\gamma_{ik}^t$, on tire pour chaque\n",
        "$i$ une réalisation $z_i$ d'une variable $Z_i$ dans $\\{1,\\dots,K\\}$ ayant\n",
        "pour loi  $(\\gamma_{i1}^t,\\dots,\\gamma_{iK}^t)$. Ces réalisations $z_i$ sont ensuite utilisées pour mettre à jour\n",
        "les moyennes, covariances et poids du GMM.\n",
        "\n",
        "- On se donne en entrée des observations $\\mathbf{x} = (x_1,\\dots,x_n)$, supposées être des réalisations i.i.d. des $X_i$, et des paramètres initiaux\n",
        "$\\theta^0 = (\\pi_k^0,\\mu_k^0,\\Sigma_k^0)_{1\\leq k\\leq K} $\n",
        "- Tant qu'on n'a pas convergé, itérer :\n",
        "  - __Etape E__: calculer pour tout $i$ dans $\\{1,\\dots,n\\}$ et\n",
        "tout $k$ dans $\\{1,\\dots,K\\}$ les valeurs\n",
        "$$\\gamma_{ik}^{t} =\\frac{\\pi_{k}^t g_{\\mu_{k}^t, \\Sigma_k^t}(x_i)}{\\sum_{j=1}^K\\pi_{j}^tg_{\\mu_{j}^t,\\Sigma_j^t}(x_i)}.$$\n",
        "  - __Etape S__:  pour tout\n",
        "$i$ dans $\\{1,\\dots,n\\}$, tirer une réalisation $z_i^t$ d'une variable $Z_i^t$ dans $\\{1,\\dots,K\\}$ ayant\n",
        "pour loi  $(\\gamma_{i1}^t,\\dots,\\gamma_{iK}^t)$\n",
        "  - __Etape M__:  mettre à jour les paramètres\n",
        "\\begin{align*}\n",
        "\\mu_k^{t+1} &= \\frac{1}{n_k^t}\\sum_{i, z_i^t = k} x_i.\n",
        "\\\\ \\Sigma_k^{t+1} &= \\frac{1}{n_k^t}\\sum_{i, z_i^t = k}(x_i- \\mu_k^{t+1}) (x_i- \\mu_k ^{t+1})^T,\n",
        "\\\\ \\pi_k^{t+1} &= \\frac {n_k^t} n,\n",
        "\\end{align*}\n",
        "où $n_k^t=|\\{i,z_i^t = k\\}|$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88d49a40-de16-436f-b4bc-9d4568e3e03e",
      "metadata": {
        "id": "88d49a40-de16-436f-b4bc-9d4568e3e03e"
      },
      "source": [
        "<font color='blue'>4. Ecrire une fonction `SEM_gaussian_mixture(x, K, niter=50)` écrite sur le modèle de la fonction précédente mais modifiée afin de réaliser l'algorithme SEM. Pour l'étape S on utilisera la fonction `numpy.random.choice`.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83b4fbac-3ad0-4991-b8f2-89f9fde35b47",
      "metadata": {
        "id": "83b4fbac-3ad0-4991-b8f2-89f9fde35b47"
      },
      "outputs": [],
      "source": [
        "# 4. fonction pour l'algorithme SEM\n",
        "# TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24317a92-b09a-4198-bae6-cfbf8b2f1253",
      "metadata": {
        "id": "24317a92-b09a-4198-bae6-cfbf8b2f1253"
      },
      "outputs": [],
      "source": [
        "K = 4\n",
        "pi, mu, Sigma, gamma = SEM_gaussian_mixture(x, K)\n",
        "\n",
        "# affichage\n",
        "z = np.argmax(gamma,axis=0)\n",
        "plot_gaussian_mixture_2d(mu,Sigma,x,z, title=\"Résultat de l'algorithme SEM\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dfad893-fa7a-459d-9ce7-e3057fde5876",
      "metadata": {
        "id": "2dfad893-fa7a-459d-9ce7-e3057fde5876"
      },
      "source": [
        "On peut remarquer que la fonction `SEM_gaussian_mixture` est assez lente. Pour améliorer ceci, voici une fonction spécifique permettant de simuler directement, à partir de vecteurs $\\gamma_i$, les variables $z_i$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd3699ea-36bb-4ad8-a9fe-107e0c327b48",
      "metadata": {
        "id": "fd3699ea-36bb-4ad8-a9fe-107e0c327b48"
      },
      "outputs": [],
      "source": [
        "def batch_random_choice(gamma):\n",
        "    K, n = gamma.shape\n",
        "    z = np.zeros(n)\n",
        "    r = np.random.rand(n)\n",
        "    a = np.zeros(n)\n",
        "    for k in range(K):\n",
        "        b = a + gamma[k,:]\n",
        "        z[(a<=r)&(r<b)] = k\n",
        "        a = b\n",
        "    return z"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6266c4c2-c2e2-4317-97c0-4a36764e185f",
      "metadata": {
        "id": "6266c4c2-c2e2-4317-97c0-4a36764e185f"
      },
      "source": [
        "<font color='blue'>5. Expliquer pourquoi la fonction `batch_random_choice` permet bien de simuler chaque $z_i$ suivant la loi donnée par $\\gamma_i$. Ecrire une fonction `SEM_gaussian_mixture_fast(x, K, niter=50)` en utilisant `batch_random_choice` pour l'étape S, puis testez là pour observer le gain de temps. Pourquoi la fonction est-elle beaucoup plus rapide?\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00630d46-32e2-4a38-89b8-8bb7f03429dc",
      "metadata": {
        "id": "00630d46-32e2-4a38-89b8-8bb7f03429dc"
      },
      "outputs": [],
      "source": [
        "# 5. fonction pour l'algorithme SEM (version rapide)\n",
        "# TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9f2506d-8ae8-41ae-9f38-b879be431f62",
      "metadata": {
        "id": "c9f2506d-8ae8-41ae-9f38-b879be431f62"
      },
      "outputs": [],
      "source": [
        "K = 4\n",
        "pi, mu, Sigma, gamma = SEM_gaussian_mixture_fast(x, K)\n",
        "\n",
        "# affichage\n",
        "z = np.argmax(gamma,axis=0)\n",
        "plot_gaussian_mixture_2d(mu,Sigma,x,z, title=\"Résultat de l'algorithme SEM\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b53aae4-ed79-4739-b09c-76cbd78a5f61",
      "metadata": {
        "id": "3b53aae4-ed79-4739-b09c-76cbd78a5f61"
      },
      "source": [
        "### Application à des données non simulées\n",
        "\n",
        "Nous allons à présent étudier des données issues de l'article suivant :\n",
        "\n",
        "- Bachrach LK, Hastie T, Wang M-C, Narasimhan B, Marcus R. *Bone Mineral Acquisition in Healthy Asian, Hispanic, Black and Caucasian Youth. A Longitudinal Study.* J Clin Endocrinol Metab (1999) 84, 4702-12.\n",
        "\n",
        "Elles représentent des mesures relatives de densité minérale osseuse spinale sur des adolescents  nord-américains. Chaque valeur est la différence de mesures prises sur deux visites consécutives, divisées par la moyenne.\n",
        "\n",
        "L'idée est de savoir si la population peut être décrite par un mélange de deux gaussiennes.\n",
        "Le but est donc d'appliquer à ces données l'algorithme EM, vérifier que le $K$ optimal de la méthode de sélection de modèle vue en cours est bien $K=2$ et proposer un clustering de ces observations.\n",
        "\n",
        "On commence par créer le tableau de donnees à partir du fichier, et afficher les points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60b87d04-08b5-4172-8e09-ea2b519cb7d4",
      "metadata": {
        "id": "60b87d04-08b5-4172-8e09-ea2b519cb7d4"
      },
      "outputs": [],
      "source": [
        "x = np.loadtxt(\"../data/densitesOs.txt\")\n",
        "n,d = x.shape\n",
        "plt.scatter(x[:,0], x[:,1], s=1, c=\"k\")\n",
        "plt.title(f\"Les {n} points du dataset\");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21a98741-a4cc-407e-824b-36872daaa7da",
      "metadata": {
        "id": "21a98741-a4cc-407e-824b-36872daaa7da"
      },
      "source": [
        "<font color='blue'>6. Appliquer la méthode EM à ces données pour des valeurs de $K$ entre $1$ et $5$, et afficher à chaque fois le résultat (on utilisera l'option `axis_equal=False` dans la fonction `plot_gaussian_mixture_2d`).\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69c8fa41-9ad6-4c72-a9be-8d6ff0b8b08a",
      "metadata": {
        "id": "69c8fa41-9ad6-4c72-a9be-8d6ff0b8b08a"
      },
      "outputs": [],
      "source": [
        "# 6. EM sur les données réelles pour K entre 1 et 5\n",
        "# TODO\n",
        "\n",
        "question_6(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "515141dc-77d1-4f2c-a5f0-c04abcd413eb",
      "metadata": {
        "id": "515141dc-77d1-4f2c-a5f0-c04abcd413eb"
      },
      "source": [
        "## Sélection de modèle\n",
        "\n",
        "Pour la sélection de modèle, il faut calculer la log-vraisemblance et la complexité du modèle.\n",
        "\n",
        "Une manière classique de choisir le nombre de composantes $K$ dans un intervalle d'entiers est d'utiliser le critère BIC qui consiste à minimiser en $K$\n",
        " $$-\\ell(x;\\widehat{\\theta}_K)+ \\frac{\\operatorname{dim}_K \\times \\log n}{2},$$\n",
        " avec\n",
        " + $\\widehat{\\theta}_K$ l'estimateur de $\\theta$ issu de l'algorithme EM dans le modèle de mélange avec $K$ classes,\n",
        " + $\\ell(x;\\hat{\\theta}_K)$ la  log-vraisemblance de l'échantillon   observé $(x_1,\\dots , x_n)$\n",
        " + $\\operatorname{dim}_K$ le nombre de degrés de liberté   du modèle de mélange utilisé avec $K$ classes (voir le cours).\n",
        "\n",
        "Le K optimal pour la sélection de modèle est l'argmin de cette fonction.\n",
        "\n",
        "<font color='blue'>7. Ecrire une fonction `log_likelihood(x,pi,mu,Sigma)` permettant de calculer la vraisemblance du GMM donné par les paramètres (pi,mu,Sigma) sur les données `x`.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c347a46-97e0-498e-8814-05a43bc0b87c",
      "metadata": {
        "id": "6c347a46-97e0-498e-8814-05a43bc0b87c"
      },
      "outputs": [],
      "source": [
        "# 7. fonction pour le calcul de la log-vraisemblance\n",
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='blue'>8. Implémenter la sélection de modèle et l'appliquer aux données précédentes pour déterminer le K optimal. Afficher également la fonction  $-\\ell(x;\\widehat{\\theta}_K)+ \\frac{\\operatorname{dim}_K \\times \\log n}{2},$ en fonction de $K$. Commentez les résultats.\n",
        "</font>"
      ],
      "metadata": {
        "id": "0w8S9hjCeWKI"
      },
      "id": "0w8S9hjCeWKI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50090e51-77ef-4852-b4b5-33362cd92549",
      "metadata": {
        "id": "50090e51-77ef-4852-b4b5-33362cd92549"
      },
      "outputs": [],
      "source": [
        "# 8. sélection de modèle par critère BIC sur les données réelles.\n",
        "# TODO\n",
        "\n",
        "selec_model(x)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}