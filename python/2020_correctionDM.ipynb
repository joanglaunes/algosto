{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\tru}{\\mathbf{t}}\n",
    "\\newcommand{\\p}{\\mathbb{P}}\n",
    "\\newcommand{\\E}{{\\mathbb{E}}}\n",
    "$\n",
    "\n",
    "\n",
    "# Correction du DM de novembre 2020 \n",
    "\n",
    "Dans ce devoir, on suppose qu'une professeure de math a posé un examen à ses étudiants dans lequel chaque question a une réponse de type ``Vrai ou Faux'' (1 ou 0). Au moment de corriger ses copies, l'enseignante s'aperçoit qu'elle a oublié quelles étaient les bonnes réponses... \n",
    "\n",
    "Elle a $N$ copies à corriger (donc $N$ étudiant.es), et l'examen comporte $M$ questions.\n",
    "\n",
    "On note $x_{n,m} \\in \\{0,1\\}$ la réponse (\\textbf{connue}) de l'étudiant.e $n$ à la question $m$, et $\\mathbf{x} = (x_{n,m})_{\\substack{n=1,\\dots,N \\\\ m=1,\\dots,M}}$ l'ensemble de toutes leurs réponses aux questions.\n",
    "\n",
    "Le but de la correction est d'estimer une note (ou un score) $\\theta_n$ pour chaque étudiant.e $n$, et de retrouver également les vraies réponses $\\mathbf{t} = (t_m)_{m=1, \\dots,M} $ (\\textbf{inconnues}) aux questions, et tout cela seulement à partir de l'ensemble $\\mathbf{x}$ des réponses des étudiant.es aux questions. Chaque réponse $t_m$ vaut $0$ ou $1$.\n",
    "\n",
    "Remarquons que si on connaissait les vraies réponses $(t_m) _{m=1\\dots M}$, alors l'étudiant.e numéro $n$ aurait une réponse juste à la question $m$ si et seulement si $x_{n,m} = t_m $, et on pourrait donc en déduire une note $\\theta_n\\in[0,1]$ pour l'étudiant.e numéro $n$ en comptant sa moyenne de réponses justes. \n",
    "\n",
    "On propose donc de modéliser ce problème d'estimation de manière similaire à celui étudié pour les mélanges de gaussiennes, où l'on a des observations (les réponses $x_{n,m}$) qui dépendent de paramètres (les notes $\\theta_n$) et de variables latentes binaires (les vraies réponses aux questions $t_m$). \n",
    "\n",
    "On va utiliser la modélisation suivante : \n",
    "- les réponses binaires $t_m$ sont des réalisations de variables $T_m$ indépendantes de même loi de Bernoulli de paramètre $0.5$, c'est-à-dire $$\\p(T_m = 1) = \\p(T_m = 0) = 0.5$$ \n",
    "- les réponses $x_{n,m}$ sont des réalisations de variables aléatoires indépendantes $X_{n,m}$, et la variable $X_{n,m}$ suit la loi conditionnelle suivante\n",
    "$$\\p[X_{n,m} = T_m|T_m] = \\theta_n\\;\\;, \\text{ et } \\p[X_{n,m} = 1-T_m|T_m] = 1-\\theta_n.$$\n",
    "\n",
    "On note $\\mathbf{\\theta} = (\\theta_1,\\dots,\\theta_N)$ l'ensemble des notes (\\textbf{inconnues}) des étudiant.es.\n",
    "On souhaite écrire un algorithme permettant d'estimer le vecteur de notes $\\mathbf{\\theta}$ et le vecteur des réponses aux questions $\\tru = (t_m)_{m=1\\dots M}$, à partir des observations $\\x$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.**Pour initialiser notre algorithme, on propose d'initialiser les réponses $t_m$ en supposant que pour chaque question $m$, une majorité d'étudiant.es répondent correctement à la question. Ecrire les estimateurs $\\hat{t}_m$ correspondant à cette hypothèse.**\n",
    "\n",
    "*REPONSE : La proportion d'étudiants qui répond $1$ à la question $m$ peut s'écrire\n",
    "$$\\frac 1 N \\sum_{n=1}^N x_{n,m}.$$\n",
    "Si cette proposition est supérieure à 0.5, il y a une majorité d'étudiants ayant répondu $1$ à la question $m$. Sinon il y a une majorité d'étudiants ayant répondu $0$ à la question $m$.\n",
    "On en déduit des estimateurs $\\hat{t}_m $ des vraies réponses aux questions \n",
    "$$\\hat{t}_m = \\begin{cases} 1 & \\text{ si }\\frac 1 N \\sum_{n=1}^N x_{n,m} > 0.5 \\;\\;\\;\\;\\; \\text{ (une majorité d'élèves à choisi la réponse 1 à la question $m$) }\\\\ 0 & \\text{ sinon.} \\end{cases}$$* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  **Montrer que la log-vraisemblance complète du problème d'estimation peut s'écrire \n",
    "  $$\\log p(\\forall m,\\; T_m = t_m, \\text{ et } \\forall n,m\\;X_{n,m} = x_{n,m} |\\mathbf{\\theta})  = M\\log 0.5 + \\sum_{n=1}^N\\sum_{m=1}^M \\log \\left(\\theta_n^{\\mathbf{1}_{x_{n,m} = t_m}} (1-\\theta_n)^{{\\mathbf{1}_{x_{n,m} = 1-t_m}}}\\right).$$**\n",
    "\n",
    "*REPONSE : Notons $F(\\x,\\tru,\\mathbf{\\theta}) = \\log p(\\forall m,\\; T_m = t_m, \\text{ et } \\forall n,m\\;X_{n,m} = x_{n,m} |\\mathbf{\\theta}) $  \n",
    "Les réponses aux différentes questions sont indépendantes, donc on peut écrire\n",
    " $$F(\\x,\\tru,\\mathbf{\\theta}) = \\log \\prod_{m=1}^M p(T_m = t_m, \\text{ et } \\forall n, \\;X_{n,m} = x_{n,m} |  \\mathbf{\\theta})$$  \n",
    " En utilisant la formule de Bayes, on obtient ensuite:  \n",
    " $$F(\\x,\\tru,\\mathbf{\\theta}) = \\log \\prod_{m=1}^M p(\\forall n, \\;X_{n,m} = x_{n,m} | T_m = t_m, \\mathbf{\\theta}) p(T_m = t_m) = M\\log 0.5 + \\log \\prod_{m=1}^M  p(\\forall n, \\;X_{n,m} = x_{n,m} | T_m = t_m, \\mathbf{\\theta}) $$\n",
    " Puis par indépendance des réponses des élèves entre elles, et en remarquant que  \n",
    " $$p(X_{n,m} = x_{n,m} | T_m = t_m, \\mathbf{\\theta}) = \\theta_n^{\\mathbf{1}_{x_{n,m} = t_m}} (1-\\theta_n)^{{\\mathbf{1}_{x_{n,m} = 1-t_m}}},$$\n",
    "  $$F(\\x,\\tru,\\mathbf{\\theta}) = M\\log 0.5 + \\log \\prod_{m=1}^M \\prod_{n=1}^N  p(X_{n,m} = x_{n,m} | T_m = t_m, \\mathbf{\\theta}) = M\\log 0.5 + \\sum_{n=1}^N\\sum_{m=1}^M \\log \\left(\\theta_n^{\\mathbf{1}_{x_{n,m} = t_m}} (1-\\theta_n)^{{\\mathbf{1}_{x_{n,m} = 1-t_m}}}\\right).$$*\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.Montrer que si $a$ et $b$ valent $0$ ou $1$, alors $\\mathbf{1}_{a = b} = ab+(1-a)(1-b)$.**\n",
    "\n",
    "*REPONSE: distinguer des différents cas...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.**Dériver l'expression de la log-vraisemblance par rapport aux notes $\\theta_n$. En déduire un estimateur $\\hat{\\theta}_n$ de la note de l'étudiant.e numéro $n$ lorsque les réponses $t_m$ sont connues. Ecrire cet estimateur $\\hat{\\theta}_n$ comme une fonction linéaire des $t_m$ grâce à l'expression montrée à la question précédente.**\n",
    "\n",
    "*REPONSE: lorsqu'on dérive $F(\\x,\\tru,\\theta)$ par rapport $\\theta_n$ on obtient \n",
    "$$\\frac{\\partial F}{\\partial \\theta_n} = \\sum_{m=1}^M \\left(\\frac{\\mathbf{1}_{x_{n,m} = t_m}}{\\theta_n } -  \\frac{\\mathbf{1}_{x_{n,m} = 1-t_m}}{1-\\theta_n }\\right)$$\n",
    "En regardant les points d'annulation de cette dérivée on en déduit l'estimateur suivant pour $\\theta_n$\n",
    "$$\\hat{\\theta}_n = \\frac 1 M \\sum_{m=1}^M \\mathbf{1}_{x_{n,m} = t_m} = \\frac 1 M \\sum_{m=1}^M  \\left(x_{n,m} t_m+ (1-x_{n,m})(1-t_m)\\right)$$*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **En pratique, on ne connaît pas les réponses $t_m$, donc on ne peut pas calculer $\\hat{\\theta}_n$ directement par maximum de vraisemblance. On va utiliser à la place des $t_m$ leurs espérances connaissant l'ensemble des observations $\\x$. Montrer que si $\\mathbf{\\theta}$ est fixé, alors\n",
    "$$\\E[T_m| \\x,\\theta] = \\frac{\\prod_{n=1}^N\\theta_n^{x_{n,m}}(1-\\theta_n)^{1-x_{n,m}}}{\\prod_{n=1}^N\\theta_n^{x_{n,m}}(1-\\theta_n)^{1-x_{n,m}} + \\prod_{n=1}^N\\theta_n^{1-x_{n,m}}(1-\\theta_n)^{x_{n,m}}}.\\label{eq:esp_tru}\n",
    "$$**\n",
    "\n",
    "*REPONSE : Remarquer que $$\\E[T_m|\\x,\\theta] = \\mathbb{P}[T_m=1| \\x,\\theta] = \\frac{\\mathbb{P}[T_m=1 \\text{ et } \\x = x|\\theta]}{\\mathbb{P}[\\x = x|\\theta]}= \\frac{\\mathbb{P}[T_m=1 \\text{ et } \\x = x|\\theta]}{\\mathbb{P}[T_m=1 \\text{ et } \\x=x |\\theta] + \\mathbb{P}[T_m=0 \\text{ et } \\x=x |\\theta]}.$$\n",
    "et conclure en notant que $$\\mathbb{P}[T_m=1 \\text{ et } \\x = x|\\theta] = \\mathbb{P}[\\x = x|T_m=1,\\theta] \\mathbb{P}[T_m=1]=0.5\\prod_{n=1}^N\\theta_n^{x_{n,m}}(1-\\theta_n)^{1-x_{n,m}}.$$* \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. En déduire un algorithme complet de type Espérance-Maximisation pour estimer les notes $\\theta = (\\theta_1,\\dots,\\theta_n)$ et les variables latentes $t_1,\\dots,t_m$ (les vraies réponses aux questions). Ecrire le pseudo-code de cet algorithme et expliquer quelle étape correspond à une maximisation et quelle étape à une espérance.**\n",
    "\n",
    "*REPONSE: PSEUDO-CODE*\n",
    "1. initialiser les estimateurs de $\\tru$\n",
    "$${t}_m = \\begin{cases} 1 & \\text{ si }\\frac 1 N \\sum_{n=1}^N x_{n,m} > 0.5 \\;\\;\\;\\;\\; \\text{ (une majorité d'élèves à choisi la réponse 1 à la question $m$) }\\\\ 0 & \\text{ sinon.} \\end{cases}$$ \n",
    "2. Pour $i$ allant de 1 à $N_{iter}$ (nombre d'itérations de l'algorithme)\n",
    "    - Etape M : mettre à jour le vecteur $\\theta$ avec pour tout $n$ allant de 1 à $N$\n",
    "      $$\\theta_n = \\frac 1 M \\sum_{m=1}^M (x_{n,m}*t_m+(1-x_{n,m})*(1-t_{n,m}))$$\n",
    "    - Etape E : mettre à jour le vecteur $ \\tru$, avec pour tout $m$ allant de 1 à $M$\n",
    "    $$ {t}_m  =  \\frac{\\prod_{n=1}^N\\theta_n^{x_{n,m}}(1-\\theta_n)^{1-x_{n,m}}}{\\prod_{n=1}^N\\theta_n^{x_{n,m}}(1-\\theta_n)^{1-x_{n,m}} + \\prod_{n=1}^N\\theta_n^{1-x_{n,m}}(1-\\theta_n)^{x_{n,m}}}$$\n",
    "3. A la fin de l'algorithme, seuiller tous les coefficients du vecteur $\\tru$ (qui prend ses valeurs entre $0$ et $1$) à $0.5$ pour obtenir un vecteur binaire.          \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie numérique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notesoublieesEM(x,niter):\n",
    "   \n",
    "    (N,M) = x.shape \n",
    "    # initialisation\n",
    "    t = np.zeros((1,M))\n",
    "    for k in range(M):   \n",
    "        t[0,k]     = np.sum(2*x[:,k]-1)>0\n",
    "\n",
    "    for iter in range(niter):\n",
    "        theta = np.mean(x*t+(1-x)*(1-t),axis=1)\n",
    "        \n",
    "        for k in range(M):   # on fait ces calculs en passant au logarithme pour des raisons de stabilité\n",
    "            a           = np.sum(x[:,k]*np.log(theta)+ (1-x[:,k])*np.log(1-theta))\n",
    "            b           = np.sum((1-x[:,k])*np.log(theta)+ x[:,k]*np.log(1-theta))\n",
    "            t[0,k]      = np.exp(a - np.log((np.exp(a)+np.exp(b))))\n",
    "        \n",
    "    t =(t>0.5)\n",
    "    return theta, t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7611940298507462\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# vérification des résultats\n",
    "x = np.load('x6.npy')       # réponses des étudiants, matrice de taille N x M (N étudiants et M questions)\n",
    "t0 = np.load('t6.npy')      # vraies réponses aux questions\n",
    "theta, t = notesoublieesEM(x,1)      # t = réponses estimées par l'algorithme après 1 itération\n",
    "print(np.mean(t0==t))                # pourcentage de réponses justes après 1 itération\n",
    "theta, t = notesoublieesEM(x,10)     # t = réponses estimées par l'algorithme après 10 itérations\n",
    "print(np.mean(t0==t))                # pourcentage de réponses justes après 10 itérations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
